# On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ

> Authors: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell

> Published: March 1st, 2021

 
## Introduction

In recent years, the field of Natural Language Processing (NLP) has witnessed a significant shift towards the development of increasingly larger language models (LMs), with BERT, GPT-2/3, and others pushing the boundaries of what's possible in understanding and generating human language. The paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell critically examines this trend, posing the essential question: **How big is too big?**

This work delves into the potential risks associated with the deployment of massive LMs, such as environmental and financial costs, biases encoded in training data, and the consequent societal impacts. By highlighting these concerns, the paper suggests researchers and developers to weigh the trade-offs between model size and utility and to explore more sustainable and ethical approaches in advancing language technology.
